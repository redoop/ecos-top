#!/usr/bin/env python3
"""
GTKWave + Claude æ³¢å½¢åŠ©æ•™
åŠŸèƒ½ï¼š
1. è‡ªåŠ¨æ‰“å¼€ GTKWave åŠ è½½æ³¢å½¢æ–‡ä»¶
2. æˆªå›¾å½“å‰å±å¹•
3. å‘é€ç»™ Claude åˆ†ææ³¢å½¢
4. è¯­éŸ³æ’­æŠ¥åˆ†æç»“æœï¼ˆå¯é€‰ï¼‰

ä½¿ç”¨æ–¹æ³•ï¼š
    python waveform_tutor.py --vcd wave.vcd --question "è¿™ä¸ªæ—¶é’Ÿä¿¡å·æ­£å¸¸å—ï¼Ÿ"
    python waveform_tutor.py --screenshot  # åªæˆªå›¾åˆ†æå½“å‰å±å¹•
"""

import os
import sys
import time
import base64
import argparse
import subprocess
from pathlib import Path

# æ£€æŸ¥ä¾èµ–
def check_dependencies():
    """æ£€æŸ¥å¹¶å®‰è£…å¿…è¦çš„ä¾èµ–"""
    required = ['anthropic', 'pillow']
    missing = []

    for pkg in required:
        try:
            __import__(pkg.replace('-', '_'))
        except ImportError:
            missing.append(pkg)

    if missing:
        print(f"æ­£åœ¨å®‰è£…ä¾èµ–: {', '.join(missing)}")
        subprocess.run([sys.executable, '-m', 'pip', 'install'] + missing,
                      capture_output=True)

    # macOS æˆªå›¾ä½¿ç”¨ç³»ç»Ÿå‘½ä»¤ï¼Œä¸éœ€è¦é¢å¤–ä¾èµ–

check_dependencies()

import anthropic
from PIL import Image

# ============ é…ç½® ============
GTKWAVE_PATH = "/opt/homebrew/bin/gtkwave"
SCREENSHOT_PATH = "/tmp/waveform_screenshot.png"
DEFAULT_VCD = "/Users/tongxiaojun/ecos-top/top/run/wave.vcd"

# API é…ç½® (å¯é€šè¿‡ç¯å¢ƒå˜é‡æˆ–å‘½ä»¤è¡Œå‚æ•°è¦†ç›–)
DEFAULT_API_BASE = "https://api.anthropic.com"  # å®˜æ–¹åœ°å€
# å¸¸ç”¨ä»£ç†åœ°å€ç¤ºä¾‹:
# - https://api.anthropic.com (å®˜æ–¹)
# - https://your-proxy.com/v1 (è‡ªå»ºä»£ç†)
# - https://openrouter.ai/api/v1 (OpenRouter)

# ============ æ ¸å¿ƒåŠŸèƒ½ ============

def open_gtkwave(vcd_file: str, save_file: str = None):
    """æ‰“å¼€ GTKWave å¹¶åŠ è½½ VCD æ–‡ä»¶"""
    cmd = [GTKWAVE_PATH, vcd_file]
    if save_file:
        cmd.extend(['-a', save_file])

    print(f"æ­£åœ¨æ‰“å¼€ GTKWave: {vcd_file}")
    process = subprocess.Popen(cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
    time.sleep(3)  # ç­‰å¾… GTKWave å¯åŠ¨
    return process

def take_screenshot() -> str:
    """æˆªå–å±å¹•"""
    # macOS ä½¿ç”¨ screencapture å‘½ä»¤
    subprocess.run(['screencapture', '-x', SCREENSHOT_PATH], check=True)
    print(f"æˆªå›¾å·²ä¿å­˜: {SCREENSHOT_PATH}")
    return SCREENSHOT_PATH

def image_to_base64(image_path: str) -> str:
    """å°†å›¾ç‰‡è½¬æ¢ä¸º base64"""
    with open(image_path, "rb") as f:
        return base64.standard_b64encode(f.read()).decode("utf-8")

def analyze_waveform(screenshot_path: str, question: str = None, api_base: str = None) -> str:
    """ä½¿ç”¨ Claude åˆ†ææ³¢å½¢å›¾"""

    api_key = os.environ.get("ANTHROPIC_API_KEY")
    if not api_key:
        return "é”™è¯¯: è¯·è®¾ç½® ANTHROPIC_API_KEY ç¯å¢ƒå˜é‡\nexport ANTHROPIC_API_KEY='ä½ çš„APIå¯†é’¥'"

    # API åœ°å€ä¼˜å…ˆçº§: å‚æ•° > ç¯å¢ƒå˜é‡ > é»˜è®¤å€¼
    base_url = api_base or os.environ.get("ANTHROPIC_API_BASE") or DEFAULT_API_BASE

    # åˆ›å»ºå®¢æˆ·ç«¯
    if base_url and base_url != "https://api.anthropic.com":
        client = anthropic.Anthropic(api_key=api_key, base_url=base_url)
        print(f"ä½¿ç”¨ API åœ°å€: {base_url}")
    else:
        client = anthropic.Anthropic(api_key=api_key)

    image_data = image_to_base64(screenshot_path)

    # æ„å»ºæç¤ºè¯
    base_prompt = """ä½ æ˜¯ä¸€ä½ç»éªŒä¸°å¯Œçš„æ•°å­—ç”µè·¯å·¥ç¨‹å¸ˆå’Œæ³¢å½¢åˆ†æä¸“å®¶ã€‚è¯·åˆ†æè¿™å¼  GTKWave æ³¢å½¢å›¾ã€‚

è¯·ç”¨ä¸­æ–‡å›ç­”ï¼ŒåŒ…æ‹¬ä»¥ä¸‹å†…å®¹ï¼š

1. **ä¿¡å·è¯†åˆ«**ï¼šè¯†åˆ«å›¾ä¸­æ˜¾ç¤ºçš„æ‰€æœ‰ä¿¡å·åç§°
2. **æ—¶é’Ÿåˆ†æ**ï¼šå¦‚æœæœ‰æ—¶é’Ÿä¿¡å·ï¼Œåˆ†æå…¶é¢‘ç‡å’Œå‘¨æœŸ
3. **ä¿¡å·å…³ç³»**ï¼šåˆ†æå„ä¿¡å·ä¹‹é—´çš„æ—¶åºå…³ç³»
4. **æ³¢å½¢ç‰¹å¾**ï¼šæŒ‡å‡ºä»»ä½•å¼‚å¸¸æˆ–å€¼å¾—æ³¨æ„çš„æ³¢å½¢ç‰¹å¾
5. **åŠŸèƒ½æ¨æ–­**ï¼šæ ¹æ®æ³¢å½¢æ¨æ–­è¿™æ®µé€»è¾‘å¯èƒ½åœ¨åšä»€ä¹ˆ

è¯·åƒè€å¸ˆæ•™å­¦ç”Ÿä¸€æ ·ï¼Œç”¨é€šä¿—æ˜“æ‡‚çš„è¯­è¨€è§£é‡Šã€‚"""

    if question:
        base_prompt += f"\n\nç”¨æˆ·ç‰¹åˆ«æƒ³äº†è§£ï¼š{question}"

    print("æ­£åœ¨åˆ†ææ³¢å½¢å›¾...")

    response = client.messages.create(
        model="claude-sonnet-4-20250514",
        max_tokens=4096,
        messages=[{
            "role": "user",
            "content": [
                {
                    "type": "image",
                    "source": {
                        "type": "base64",
                        "media_type": "image/png",
                        "data": image_data,
                    },
                },
                {
                    "type": "text",
                    "text": base_prompt
                }
            ],
        }]
    )

    return response.content[0].text

def text_to_speech(text: str):
    """ä½¿ç”¨ macOS ç³»ç»Ÿè¯­éŸ³æœ—è¯»æ–‡æœ¬"""
    # ä½¿ç”¨ä¸­æ–‡è¯­éŸ³
    subprocess.run(['say', '-v', 'Ting-Ting', text[:500]])  # é™åˆ¶é•¿åº¦é¿å…å¤ªé•¿

def interactive_mode(vcd_file: str = None, api_base: str = None):
    """äº¤äº’æ¨¡å¼ï¼šæŒç»­é—®ç­”"""
    print("\n" + "="*50)
    print("  GTKWave æ³¢å½¢åŠ©æ•™ - äº¤äº’æ¨¡å¼")
    print("="*50)
    print("\nå‘½ä»¤:")
    print("  screenshot / s  - æˆªå›¾å¹¶åˆ†æ")
    print("  open <file>     - æ‰“å¼€æ–°çš„ VCD æ–‡ä»¶")
    print("  voice on/off    - å¼€å¯/å…³é—­è¯­éŸ³æ’­æŠ¥")
    print("  api <url>       - æ›´æ”¹ API åœ°å€")
    print("  quit / q        - é€€å‡º")
    print("\nç›´æ¥è¾“å…¥é—®é¢˜ï¼Œå°†åŸºäºå½“å‰æˆªå›¾å›ç­”\n")

    # æ˜¾ç¤ºå½“å‰ API é…ç½®
    current_api = api_base or os.environ.get("ANTHROPIC_API_BASE") or DEFAULT_API_BASE
    print(f"å½“å‰ API åœ°å€: {current_api}\n")

    gtkwave_process = None
    voice_enabled = False

    if vcd_file:
        gtkwave_process = open_gtkwave(vcd_file)

    while True:
        try:
            user_input = input("\nğŸ“ è¯·è¾“å…¥é—®é¢˜æˆ–å‘½ä»¤: ").strip()

            if not user_input:
                continue

            if user_input.lower() in ['quit', 'q', 'exit']:
                print("å†è§ï¼")
                break

            if user_input.lower() in ['screenshot', 's']:
                take_screenshot()
                result = analyze_waveform(SCREENSHOT_PATH, api_base=api_base)
                print("\n" + "="*50)
                print(result)
                print("="*50)
                if voice_enabled:
                    text_to_speech(result)
                continue

            if user_input.lower().startswith('open '):
                new_file = user_input[5:].strip()
                if os.path.exists(new_file):
                    if gtkwave_process:
                        gtkwave_process.terminate()
                    gtkwave_process = open_gtkwave(new_file)
                else:
                    print(f"æ–‡ä»¶ä¸å­˜åœ¨: {new_file}")
                continue

            if user_input.lower().startswith('api '):
                api_base = user_input[4:].strip()
                print(f"API åœ°å€å·²æ›´æ”¹ä¸º: {api_base}")
                continue

            if user_input.lower() == 'voice on':
                voice_enabled = True
                print("è¯­éŸ³æ’­æŠ¥å·²å¼€å¯")
                continue

            if user_input.lower() == 'voice off':
                voice_enabled = False
                print("è¯­éŸ³æ’­æŠ¥å·²å…³é—­")
                continue

            # ç”¨æˆ·æé—® - æˆªå›¾ååˆ†æ
            take_screenshot()
            result = analyze_waveform(SCREENSHOT_PATH, user_input, api_base=api_base)
            print("\n" + "="*50)
            print(result)
            print("="*50)
            if voice_enabled:
                text_to_speech(result)

        except KeyboardInterrupt:
            print("\nå†è§ï¼")
            break
        except Exception as e:
            print(f"é”™è¯¯: {e}")

    if gtkwave_process:
        gtkwave_process.terminate()

def main():
    parser = argparse.ArgumentParser(description='GTKWave + Claude æ³¢å½¢åŠ©æ•™')
    parser.add_argument('--vcd', type=str, help='VCD æ³¢å½¢æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--screenshot', action='store_true', help='åªæˆªå›¾åˆ†æå½“å‰å±å¹•')
    parser.add_argument('--question', '-q', type=str, help='å…·ä½“é—®é¢˜')
    parser.add_argument('--interactive', '-i', action='store_true', help='äº¤äº’æ¨¡å¼')
    parser.add_argument('--voice', '-v', action='store_true', help='å¯ç”¨è¯­éŸ³æ’­æŠ¥')
    parser.add_argument('--list', '-l', action='store_true', help='åˆ—å‡ºå¯ç”¨çš„ VCD æ–‡ä»¶')
    parser.add_argument('--api-base', '--api', type=str,
                       help='API åœ°å€ (é»˜è®¤: https://api.anthropic.com)')
    parser.add_argument('--model', '-m', type=str, default='claude-sonnet-4-20250514',
                       help='æ¨¡å‹åç§° (é»˜è®¤: claude-sonnet-4-20250514)')

    args = parser.parse_args()

    # è®¾ç½® API åœ°å€åˆ°ç¯å¢ƒå˜é‡ï¼ˆæ–¹ä¾¿åç»­ä½¿ç”¨ï¼‰
    if args.api_base:
        os.environ["ANTHROPIC_API_BASE"] = args.api_base

    # åˆ—å‡º VCD æ–‡ä»¶
    if args.list:
        vcd_files = list(Path('/Users/tongxiaojun/ecos-top').rglob('*.vcd'))
        print("\nå¯ç”¨çš„ VCD æ–‡ä»¶:")
        for i, f in enumerate(vcd_files, 1):
            size = f.stat().st_size / 1024 / 1024
            print(f"  {i}. {f} ({size:.1f} MB)")
        return

    # äº¤äº’æ¨¡å¼
    if args.interactive:
        interactive_mode(args.vcd, api_base=args.api_base)
        return

    # åªæˆªå›¾åˆ†æ
    if args.screenshot:
        take_screenshot()
        result = analyze_waveform(SCREENSHOT_PATH, args.question, api_base=args.api_base)
        print("\n" + result)
        if args.voice:
            text_to_speech(result)
        return

    # æ‰“å¼€ VCD æ–‡ä»¶å¹¶åˆ†æ
    vcd_file = args.vcd or DEFAULT_VCD
    if not os.path.exists(vcd_file):
        print(f"æ–‡ä»¶ä¸å­˜åœ¨: {vcd_file}")
        print("ä½¿ç”¨ --list æŸ¥çœ‹å¯ç”¨çš„ VCD æ–‡ä»¶")
        return

    process = open_gtkwave(vcd_file)

    print("\nGTKWave å·²æ‰“å¼€ï¼Œè¯·è°ƒæ•´æ³¢å½¢è§†å›¾åæŒ‰ Enter æˆªå›¾åˆ†æ...")
    input()

    take_screenshot()
    result = analyze_waveform(SCREENSHOT_PATH, args.question, api_base=args.api_base)
    print("\n" + result)

    if args.voice:
        text_to_speech(result)

    print("\næŒ‰ Enter å…³é—­ GTKWave...")
    input()
    process.terminate()

if __name__ == "__main__":
    main()
